{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import jupyanno as ja\n",
        "from jupyanno.sheets import get_task_sheet\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "import seaborn as sns\n",
        "from IPython.display import FileLink, HTML\n",
        "ja.setup_appmode()\n",
        "USERNAME = ja.get_user_id()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "task_data = ja.read_task_file('../task.json')\n",
        "annot_df = ja.read_annotation(get_task_sheet(task_data))\n",
        "print('Showing most recent 3 annotations')\n",
        "annot_df.tail(3)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "annot_df = get_task_sheet(task_data)\n",
        "# anonymize before splitting\n",
        "anon_ids = {k: 'Anonymous_User_{}'.format(i) for i,k in enumerate(np.unique(annot_df['annotator'].values))}\n",
        "annot_df['annotator'] = annot_df.apply(lambda c_row: USERNAME \n",
        "                                     if c_row['annotator']==USERNAME else \n",
        "                                     anon_ids.get(c_row['annotator']),axis=1)\n",
        "annot_df = ja.read_annotation(annot_df)\n",
        "print('Showing most recent 3 annotations')\n",
        "annot_df.tail(3)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "annot_df.groupby(['annotator_class', 'annotator_name']).\\\n",
        "    agg({'viewing_time': ['sum', 'mean'], 'label': len}).\\\n",
        "    reset_index().\\\n",
        "    rename({'label': 'count'}, axis=1).\\\n",
        "    round(2).\\\n",
        "    sort_values(('count', 'len'), ascending=False).\\\n",
        "    style.\\\n",
        "    bar(color='#d65f5f')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "results_list = []\n",
        "# we need to combine the results from binary class and multiclass problems distinctly\n",
        "\n",
        "for c_mode, group_annot_df in annot_df.groupby('annotation_mode'):\n",
        "    group_annot_df = group_annot_df.copy()\n",
        "    group_annot_df['answer'] = group_annot_df['label']\n",
        "    if c_mode == 'BinaryClass':\n",
        "        # make the binary result look like a more standard question\n",
        "        group_annot_df['label'] = group_annot_df.apply(\n",
        "            lambda c_row: c_row['task'] if c_row['label'] == 'Yes' else None, 1)\n",
        "    elif c_mode == 'MultiClass':\n",
        "        pass\n",
        "    else:\n",
        "        print('Dashboard does not support {} problems yet!'.format(c_mode))\n",
        "\n",
        "    c_results_df = pd.merge(group_annot_df,\n",
        "                            task_data.data_df,\n",
        "                            how='left',\n",
        "                            left_on='item_id',\n",
        "                            right_on=task_data.image_key_col)\n",
        "\n",
        "    if c_mode == 'BinaryClass':\n",
        "        c_results_df['correct'] = c_results_df.apply(lambda x: ja.binary_correct(x, task_data.label_col), 1)\n",
        "    elif c_mode == 'MultiClass':\n",
        "        c_results_df['correct'] = c_results_df.apply(\n",
        "            lambda c_row: c_row['label'] == c_row[task_data.label_col], 1)\n",
        "\n",
        "    results_list += [c_results_df]\n",
        "results_df = pd.concat(results_list)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# My Personal Results\n",
        "Here we show the questions you directly answered"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "my_answers = results_df[results_df['annotator'] == USERNAME]['correct'].values\n",
        "ja.show_my_result([USERNAME], [my_answers.sum()], my_answers.shape[0])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "my_answers_df = results_df[results_df['annotator'] == USERNAME][[\n",
        "    'Timestamp', 'correct', task_data.image_key_col, 'task', 'answer', task_data.label_col]].copy()\n",
        "\n",
        "my_answers_df = my_answers_df.sort_values(\n",
        "    ['Timestamp'], ascending=True).drop(['Timestamp'], 1)\n",
        "my_answers_df[task_data.image_key_col] = my_answers_df[task_data.image_key_col].map(\n",
        "    lambda x: ja.path_to_img(os.path.join(task_data.base_img_dir, x)))\n",
        "html_output = ja.raw_html_render(my_answers_df.rename({task_data.image_key_col: 'Image',\n",
        "                                           'task': 'Question',\n",
        "                                           'answer': 'Your Answer',\n",
        "                                           task_data.label_col: 'Real Answer'}, axis=1))\n",
        "with open('download_full_table.html', 'w') as f:\n",
        "    f.write(html_output)\n",
        "    display(FileLink(f.name))\n",
        "\nHTML(html_output)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overall Results\n",
        "We can show the overall results by person / type of user"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "results_df.groupby(['annotator_class', 'annotator_name']).\\\n",
        "    agg({'viewing_time': 'mean',\n",
        "         'label': len,\n",
        "         'correct': lambda x: 100*np.mean(x),\n",
        "         'answer_negativity': lambda x: 100*np.mean(x)}).\\\n",
        "    reset_index().\\\n",
        "    rename({'label': 'Count',\n",
        "            'correct': 'Accuracy (%)',\n",
        "            'viewing_time': 'Average Viewing Time (s)',\n",
        "            'annotator_class': 'Type of User',\n",
        "            'annotator_name': 'Name',\n",
        "            'answer_negativity': 'Negative Responses (%)'}, axis=1).\\\n",
        "    round(1).\\\n",
        "    sort_values('Accuracy (%)', ascending=False).\\\n",
        "    style.\\\n",
        "    background_gradient(cmap='hot', low=.5, high=0).\\\n",
        "    set_properties(**{'font-size': '12pt'})"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Disease to be identified \n",
        "Here we show the breakdown based on which condition the patient actually had"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "results_df.groupby(task_data.label_col).\\\n",
        "    agg({'viewing_time': 'mean', 'label': len, 'correct': lambda x: 100*np.mean(x)}).\\\n",
        "    reset_index().\\\n",
        "    rename({'label': 'count',\n",
        "            'correct': 'Accuracy (%)',\n",
        "            'viewing_time': 'Average Viewing Time (s)'\n",
        "            }, axis=1).\\\n",
        "    round(1).\\\n",
        "    sort_values('Accuracy (%)', ascending=False).\\\n",
        "    style.\\\n",
        "    background_gradient(cmap='hot', low=.5, high=0).\\\n",
        "    set_properties(**{'font-size': '12pt'})"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question Asked\n",
        "Finally we show the breakdown based on the question asked"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "results_df.groupby('task').\\\n",
        "    agg({'viewing_time': 'mean', 'label': len, 'correct': lambda x: 100*np.mean(x)}).\\\n",
        "    reset_index().\\\n",
        "    rename({'label': 'count',\n",
        "            'correct': 'Accuracy (%)',\n",
        "            'viewing_time': 'Average Viewing Time (s)',\n",
        "            'task': 'Question Asked'\n",
        "            }, axis=1).\\\n",
        "    round(1).\\\n",
        "    sort_values('Accuracy (%)', ascending=False).\\\n",
        "    style.\\\n",
        "    background_gradient(cmap='hot', low=.5, high=0).\\\n",
        "    set_properties(**{'font-size': '12pt'})"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results vs Random Guesses\n",
        "Show the results vs Random Guesses for each user"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "res_df = results_df.groupby(['annotator', 'annotator_name']).\\\n",
        "    agg({'Timestamp': 'count', 'correct': 'sum'}).\\\n",
        "    reset_index().\\\n",
        "    rename({'Timestamp': 'count'}, axis=1)\n",
        "for count, c_df in res_df.groupby('count'):\n",
        "    ax1 = ja.show_my_result(c_df['annotator_name'].values.tolist(),\n",
        "                         c_df['correct'].values.astype(int).tolist(),\n",
        "                         count)\n",
        "    ax1.set_title('{} questions answered'.format(count))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python3"
    },
    "nteract": {
      "version": "0.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}