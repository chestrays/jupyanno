{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotation Dashboard\n",
    "Here is an example of a full-fledged annotation dashboard to see detailed information on the cases annotated and the breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import jupyanno as ja\n",
    "from jupyanno.sheets import get_task_sheet\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "ja.setup_appmode()\n",
    "USERNAME = ja.get_app_user_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_data = ja.read_task_file('../task.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot_df = ja.read_annotation(get_task_sheet(task_data))\n",
    "print('Showing most recent 3 annotations')\n",
    "annot_df.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot_df.groupby(['annotator_class', 'annotator_name']).\\\n",
    "    agg({'viewing_time': ['sum', 'mean'], 'label': len}).\\\n",
    "    reset_index().\\\n",
    "    rename({'label': 'count'}, axis=1).\\\n",
    "    round(2).\\\n",
    "    sort_values(('count', 'len'), ascending=False).\\\n",
    "    style.\\\n",
    "    bar(color='#d65f5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list = []\n",
    "# we need to combine the results from binary class and multiclass problems distinctly\n",
    "\n",
    "for c_mode, group_annot_df in annot_df.groupby('annotation_mode'):\n",
    "    group_annot_df = group_annot_df.copy()\n",
    "    group_annot_df['answer'] = group_annot_df['label']\n",
    "    if c_mode == 'BinaryClass':\n",
    "        # make the binary result look like a more standard question\n",
    "        group_annot_df['label'] = group_annot_df.apply(\n",
    "            lambda c_row: c_row['task'] if c_row['label'] == 'Yes' else None, 1)\n",
    "    elif c_mode == 'MultiClass':\n",
    "        pass\n",
    "    else:\n",
    "        print('Dashboard does not support {} problems yet!'.format(c_mode))\n",
    "\n",
    "    c_results_df = pd.merge(group_annot_df,\n",
    "                            task_data.data_df,\n",
    "                            how='left',\n",
    "                            left_on='item_id',\n",
    "                            right_on=task_data.image_key_col)\n",
    "\n",
    "    if c_mode == 'BinaryClass':\n",
    "        c_results_df['correct'] = c_results_df.apply(lambda x: ja.task.binary_correct(x, task_data.label_col), 1)\n",
    "    elif c_mode == 'MultiClass':\n",
    "        c_results_df['correct'] = c_results_df.apply(\n",
    "            lambda c_row: c_row['label'] == c_row[task_data.label_col], 1)\n",
    "\n",
    "    results_list += [c_results_df]\n",
    "results_df = pd.concat(results_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Personal Results\n",
    "Here we show the questions you directly answered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_answers = results_df[results_df['annotator'] == USERNAME]['correct'].values\n",
    "ja.task.show_my_result([USERNAME], [my_answers.sum()], my_answers.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_answers_df = results_df[results_df['annotator'] == USERNAME][[\n",
    "    'Timestamp', 'correct', task_data.image_key_col, 'task', 'answer', task_data.label_col]].copy()\n",
    "\n",
    "my_answers_df = my_answers_df.sort_values(\n",
    "    ['Timestamp'], ascending=True).drop(['Timestamp'], 1)\n",
    "my_answers_df[task_data.image_key_col] = my_answers_df[task_data.image_key_col].map(\n",
    "    lambda x: ja.utils.path_to_img(os.path.join(task_data.base_img_dir, x)))\n",
    "out_html = ja.utils.raw_html_render(my_answers_df.rename({task_data.image_key_col: 'Image',\n",
    "                                           'task': 'Question',\n",
    "                                           'answer': 'Your Answer',\n",
    "                                           task_data.label_col: 'Real Answer'}, axis=1))\n",
    "from IPython.display import HTML\n",
    "HTML(out_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall Results\n",
    "We can show the overall results by person / type of user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.groupby(['annotator_class', 'annotator_name']).\\\n",
    "    agg({'viewing_time': 'mean',\n",
    "         'label': len,\n",
    "         'correct': lambda x: 100*np.mean(x),\n",
    "         'answer_negativity': lambda x: 100*np.mean(x)}).\\\n",
    "    reset_index().\\\n",
    "    rename({'label': 'Count',\n",
    "            'correct': 'Accuracy (%)',\n",
    "            'viewing_time': 'Average Viewing Time (s)',\n",
    "            'annotator_class': 'Type of User',\n",
    "            'annotator_name': 'Name',\n",
    "            'answer_negativity': 'Negative Responses (%)'}, axis=1).\\\n",
    "    round(1).\\\n",
    "    sort_values('Accuracy (%)', ascending=False).\\\n",
    "    style.\\\n",
    "    background_gradient(cmap='hot', low=.5, high=0).\\\n",
    "    set_properties(**{'font-size': '12pt'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disease to be identified \n",
    "Here we show the breakdown based on which condition the patient actually had"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.groupby(task_data.label_col).\\\n",
    "    agg({'viewing_time': 'mean', 'label': len, 'correct': lambda x: 100*np.mean(x)}).\\\n",
    "    reset_index().\\\n",
    "    rename({'label': 'count',\n",
    "            'correct': 'Accuracy (%)',\n",
    "            'viewing_time': 'Average Viewing Time (s)'\n",
    "            }, axis=1).\\\n",
    "    round(1).\\\n",
    "    sort_values('Accuracy (%)', ascending=False).\\\n",
    "    style.\\\n",
    "    background_gradient(cmap='hot', low=.5, high=0).\\\n",
    "    set_properties(**{'font-size': '12pt'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Asked\n",
    "Finally we show the breakdown based on the question asked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.groupby('task').\\\n",
    "    agg({'viewing_time': 'mean', 'label': len, 'correct': lambda x: 100*np.mean(x)}).\\\n",
    "    reset_index().\\\n",
    "    rename({'label': 'count',\n",
    "            'correct': 'Accuracy (%)',\n",
    "            'viewing_time': 'Average Viewing Time (s)',\n",
    "            'task': 'Question Asked'\n",
    "            }, axis=1).\\\n",
    "    round(1).\\\n",
    "    sort_values('Accuracy (%)', ascending=False).\\\n",
    "    style.\\\n",
    "    background_gradient(cmap='hot', low=.5, high=0).\\\n",
    "    set_properties(**{'font-size': '12pt'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results vs Random Guesses\n",
    "Show the results vs Random Guesses for each user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = results_df.groupby(['annotator', 'annotator_name']).\\\n",
    "    agg({'Timestamp': 'count', 'correct': 'sum'}).\\\n",
    "    reset_index().\\\n",
    "    rename({'Timestamp': 'count'}, axis=1)\n",
    "for count, c_df in res_df.groupby('count'):\n",
    "    ax1 = ja.task.show_my_result(c_df['annotator_name'].values.tolist(),\n",
    "                         c_df['correct'].values.astype(int).tolist(),\n",
    "                         count)\n",
    "    ax1.set_title('{} questions answered'.format(count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Results\n",
    "The full results can be exported and viewed in Excel or Google Sheets and analyzed even further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('results.csv', index=False)\n",
    "from IPython.display import FileLink\n",
    "print('Download Results')\n",
    "FileLink('results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View by Images\n",
    "Here we show the summarized physicians answers to each question by image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "def magnify(th_size='4pt'):\n",
    "    return [dict(selector=\"th\",\n",
    "                 props=[(\"font-size\", th_size)]),\n",
    "            dict(selector=\"td\",\n",
    "                 props=[('padding', \"0em 0em\")]),\n",
    "            dict(selector=\"th:hover\",\n",
    "                 props=[(\"font-size\", \"12pt\")]),\n",
    "            dict(selector=\"tr:hover td:hover\",\n",
    "                 props=[('max-width', '200px'),\n",
    "                        ('font-size', '12pt')])\n",
    "]\n",
    "\n",
    "with pd.option_context('display.precision', 2):\n",
    "    display(results_df.pivot_table(index=[task_data.label_col, 'item_id'], \n",
    "                           columns =['task'], \n",
    "                           values='answer_negativity', \n",
    "                           aggfunc=lambda x: 1-np.mean(x)).\\\n",
    "            style.\\\n",
    "            set_properties(**{'max-width': '80px', 'font-size': '1pt'}).\\\n",
    "            background_gradient(cmap='viridis', low=1, high=0).\\\n",
    "            highlight_null('black').\\\n",
    "            set_table_styles(magnify())\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "get_most_common = lambda items: max(Counter(items).items(), key = lambda x: x[1])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df['_idx'] = pd.Categorical(results_df['item_id']).codes\n",
    "pivot_df = results_df[~results_df['label'].isin(['None'])].\\\n",
    "    groupby([task_data.label_col,'annotator', 'annotator_name', '_idx']).\\\n",
    "    agg({'label': get_most_common}).\\\n",
    "    reset_index().\\\n",
    "    pivot_table(columns=[task_data.label_col, '_idx'], \n",
    "                       index =['annotator_name'], \n",
    "                       values='label', \n",
    "                       aggfunc=get_most_common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_enc = {k: i for i, k in enumerate(set(pivot_df.melt()['value'].dropna()))}\n",
    "c_palette = sns.color_palette(\"Set2\", len(lab_enc)+1)\n",
    "c_css_p = (np.array(c_palette)*255).clip(0, 255).astype(np.uint8)\n",
    "def color_by_label(val):\n",
    "    c_idx = lab_enc.get(val, -1)+1\n",
    "    color = c_css_p[c_idx]\n",
    "    return 'background-color: rgb({}, {}, {})'.format(*color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'Finding': list(lab_enc.keys())}).style.applymap(color_by_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_df.\\\n",
    "    style.\\\n",
    "    applymap(color_by_label).\\\n",
    "    set_properties(**{'max-width': '120px', 'font-size': '1pt'}).\\\n",
    "    highlight_null('black').\\\n",
    "    set_table_styles(magnify('5pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th_props = [\n",
    "  ('font-size', '11px'),\n",
    "  ('text-align', 'center'),\n",
    "  ('font-weight', 'bold'),\n",
    "  ('color', '#6d6d6d'),\n",
    "  ('background-color', '#f7f7f9')\n",
    "  ]\n",
    "\n",
    "# Set CSS properties for td elements in dataframe\n",
    "td_props = [\n",
    "  ('font-size', '11px')\n",
    "  ]\n",
    "\n",
    "# Set table styles\n",
    "styles = [\n",
    "  dict(selector=\"th\", props=th_props),\n",
    "  dict(selector=\"td\", props=td_props)\n",
    "  ]\n",
    "for c_label, raw_rows in results_df.groupby(task_data.label_col):\n",
    "    display(Markdown('## {}'.format(c_label)))\n",
    "    c_rows = raw_rows.copy()\n",
    "    c_rows['answer_value'] = 1.0-c_rows['answer_negativity']\n",
    "    display(c_rows.pivot_table(index=['item_id'], \n",
    "                       columns =['task', 'annotator_name'], \n",
    "                       values='answer_value', \n",
    "                       aggfunc='mean').\\\n",
    "        style.\\\n",
    "        background_gradient(cmap='viridis', low=1, high=0).\\\n",
    "        highlight_null('black').\\\n",
    "        set_properties(**{'font-size': '12pt'}).\\\n",
    "            set_table_styles(styles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Interaction\n",
    "Here we look at the interaction that took place with the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "# flatten out all the zoom results\n",
    "has_zoom = results_df['viewing_info_dict'].map(lambda x: 'zoom' in x)\n",
    "zoom_df = results_df[has_zoom].copy()\n",
    "print(zoom_df.shape[0], 'results with zoom information')\n",
    "if zoom_df.shape[0]>0:\n",
    "    zoom_df['zoom'] = zoom_df.apply(\n",
    "        lambda c_row: [dict(y, img_idx=c_row.name)\n",
    "                       for y in c_row['viewing_info_dict']['zoom']\n",
    "                       if ('x' in y) and ('y' in y)],\n",
    "        axis=1\n",
    "    )\n",
    "    zooms_df = pd.DataFrame(list(chain(*zoom_df['zoom'].values.tolist())))\n",
    "    zooms_df['img_cat'] = pd.factorize(zooms_df['img_idx'])[0]\n",
    "else:\n",
    "    zooms_df = pd.DataFrame(dict(x=[], y=[], img_idx=[], img_cat=[]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Rectangle\n",
    "for col_name in 'xy':\n",
    "    for i in range(2):\n",
    "        zooms_df['{}_{}'.format(col_name, i)] = zooms_df[col_name].map(\n",
    "            lambda x: x[i])\n",
    "    zooms_df['d_{}'.format(col_name, i)] = zooms_df[col_name].map(\n",
    "        lambda x: max(x)-min(x))\n",
    "fig, ax1 = plt.subplots(1, 1, figsize=(10, 10))\n",
    "if zooms_df.shape[0]>1:\n",
    "    ax1.set_xlim([zooms_df['x_0'].min(), zooms_df['x_1'].max()])\n",
    "    ax1.set_ylim([zooms_df['y_0'].min(), zooms_df['y_1'].max()])\n",
    "max_cat = zooms_df['img_cat'].max()\n",
    "\n",
    "\n",
    "for c_idx, c_rows in zooms_df.groupby('img_idx'):\n",
    "    bonus_args = {'label': c_idx}\n",
    "    for _, c_row in c_rows.iterrows():\n",
    "        ax1.add_patch(Rectangle(xy=(c_row['x_0'], c_row['y_0']),\n",
    "                                width=c_row['d_x'], height=c_row['d_y'],\n",
    "                                color=plt.cm.hot(c_row['img_cat']/max_cat),\n",
    "                                **bonus_args,\n",
    "                                alpha=0.1))\n",
    "        bonus_args = {}\n",
    "ax1.set_title('Map of Zoom Regions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "ax1.hist((zooms_df['d_x']*zooms_df['d_y'])/1e3, bins=20)\n",
    "ax1.set_title('kilopixels examined')\n",
    "for c_idx, c_rows in zooms_df.groupby('img_idx'):\n",
    "    ax2.semilogy((c_rows['d_x'].values*c_rows['d_y'].values),\n",
    "                 label='Img: {}'.format(c_idx))\n",
    "ax2.set_xlabel('Interaction Step')\n",
    "ax2.set_ylabel('Pixels Visible')\n",
    "ax2.set_title('Pixels visible vs Interaction Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, m_axs = plt.subplots(3, 3, figsize=(15, 15), dpi=300)\n",
    "[c_ax.axis('off') for c_ax in m_axs.flatten()]\n",
    "for c_ax, (img_idx, zoom_rows) in zip(m_axs.flatten(),\n",
    "                                      zooms_df.groupby('img_idx')):\n",
    "    img_row = results_df.loc[img_idx]\n",
    "    c_img_path = os.path.join(task_data.base_img_dir, img_row[task_data.image_key_col])\n",
    "    img_data = ja.utils.load_image_multiformat(c_img_path)\n",
    "    c_ax.imshow(img_data)\n",
    "    for idx, (_, c_row) in enumerate(zoom_rows.iterrows()):\n",
    "        c_ax.add_patch(Rectangle(xy=(c_row['x_0'], \n",
    "                                     img_data.shape[1]-c_row['y_1']),\n",
    "                                 width=c_row['d_x'], height=c_row['d_y'],\n",
    "                                 color=plt.cm.viridis(idx/zoom_rows.shape[0]),\n",
    "                                 label=str(idx),\n",
    "                                 linewidth=2,\n",
    "                                 fill=False,\n",
    "                                 alpha=0.5))\n",
    "    c_ax.legend()\n",
    "    legend = c_ax.legend(loc='upper right', shadow=True, frameon=True)\n",
    "    # black on black is hard to read\n",
    "    legend.get_frame().set_facecolor('#FFFFFF')\n",
    "    c_ax.set_title(\n",
    "        '{annotator}, Task:{task}\\nTime: {viewing_time:2.1f}s, Correct: {correct}'.format(**img_row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Fun Graphs\n",
    "Just a few other graphs to look at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x='time',\n",
    "           y='viewing_time',\n",
    "           col='annotator_name',\n",
    "           sharex=False,\n",
    "           sharey=False,\n",
    "           col_wrap=4,\n",
    "           ci=None,\n",
    "           data=results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(1, 1, figsize=(20, 5))\n",
    "sns.swarmplot(x='annotator_name',\n",
    "              y='correct',\n",
    "              hue='answer',\n",
    "              size=5,\n",
    "              ax=ax1,\n",
    "              data=results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
